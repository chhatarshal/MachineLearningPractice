# -*- coding: utf-8 -*-
"""Practice_work_in_progress.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ij3luk-pPi0ehImUCWhFpETKQvwAwxSN

In this notebook we will train and fine tune our model using Transfer Learning technique
"""

import zipfile

"""We will fetch only 10% of food vision data and then we will train our model"""

!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip

!wget https://raw.githubusercontent.com/chhatarshal/MachineLearningPractice/main/tensorflow/helper/helper_functions.py

from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir

unzip_data('10_food_classes_10_percent.zip')

train_dir = "10_food_classes_10_percent/train"
test_dir = "10_food_classes_10_percent/test"

import tensorflow as tf

IMG_SIZE = (224, 224)
BATCH_SIZE = 32

train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(
    directory=train_dir,
    image_size = IMG_SIZE,
    label_mode = "categorical",
    batch_size = BATCH_SIZE
)

test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(
    directory= test_dir,
    image_size = IMG_SIZE,
    label_mode = "categorical",
    batch_size = BATCH_SIZE
)

base_model = tf.keras.applications.MobileNetV2(include_top=False)

base_model.trainable = False
inputs = tf.keras.layers.Input(shape=(224, 224, 3), name="input_layer")

x = base_model(inputs)

x = tf.keras.layers.GlobalAveragePooling2D(name="global_average_pooling_layers")(x)

outputs = tf.keras.layers.Dense(10, activation="softmax", name="output_layer")(x)

model_0 = tf.keras.Model(inputs, outputs)

model_0.compile(loss="categorical_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics = ["accuracy"])

history_10_percent = model_0.fit(train_data_10_percent, epochs=5,
                                 steps_per_epoch =len(train_data_10_percent))

model_0.evaluate(test_data_10_percent)

base_model = tf.keras.applications.EfficientNetB0(include_top=False)
base_model.trainable = False
inputs = tf.keras.layers.Input(shape=(224, 224, 3), name="input_layer")
x = base_model(inputs)
x = tf.keras.layers.GlobalAveragePooling2D(name="global_average_pooling_layers")(x)
outputs = tf.keras.layers.Dense(10, activation="softmax", name="output_layer")(x)
model_1 = tf.keras.Model(inputs, outputs)
model_1.compile(loss="categorical_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics = ["accuracy"])
#model_1.evaluate(test_data_10_percent)

history_10_percent_eff = model_1.fit(train_data_10_percent, epochs=5,
                                 steps_per_epoch =len(train_data_10_percent))

model_1.evaluate(test_data_10_percent)

base_model = tf.keras.applications.EfficientNetB7(include_top=False)
base_model.trainable = False
inputs = tf.keras.layers.Input(shape=(224, 224, 3), name="input_layer")
x = base_model(inputs)
x = tf.keras.layers.GlobalAveragePooling2D(name="global_average_pooling_layers")(x)
outputs = tf.keras.layers.Dense(10, activation="softmax", name="output_layer")(x)
model_1 = tf.keras.Model(inputs, outputs)
model_1.compile(loss="categorical_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics = ["accuracy"])
#model_1.evaluate(test_data_10_percent)

history_10_percent_eff = model_1.fit(train_data_10_percent, epochs=5,
                                 steps_per_epoch =len(train_data_10_percent))

"""**Here we compare MobileNetV2, EfficientNetB0, EfficientNetB7 and we observed MobileNetV2 perform worst compare to EfficientNetB0 and EfficientNetB7**

MobileNetV2 : **0.2848**

EfficientNetB0 : **0.8693**

EfficientNetB7 : **0.8773**
"""

